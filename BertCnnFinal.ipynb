{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BertCnnFinal.ipynb",
      "provenance": [],
      "mount_file_id": "1vj3Z4UpPYqCamgeWQiN11TteiazAwTt5",
      "authorship_tag": "ABX9TyNAH18fNerSp+olcSBcN+Sm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZeroxTM/BERT-CNN-Fine-Tuning-For-Hate-Speech-Detection-in-Online-Social-Media/blob/main/BertCnnFinal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KuTl1VnNfsX",
        "outputId": "8ecf776f-dbbc-4e5b-96f3-4158f3459a32"
      },
      "source": [
        "\"\"\"\r\n",
        " @Time : 15/12/2020 19:01\r\n",
        " @Author : Alaa Grable\r\n",
        " \"\"\"\r\n",
        "\r\n",
        "!pip install transformers==3.0.0\r\n",
        "!pip install emoji\r\n",
        "import gc\r\n",
        "#import os\r\n",
        "import emoji as emoji\r\n",
        "import re\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import classification_report, accuracy_score\r\n",
        "from transformers import AutoModel\r\n",
        "from transformers import BertModel, BertTokenizer\r\n",
        "\r\n",
        "class BERT_Arch(nn.Module):\r\n",
        "\r\n",
        "    def __init__(self, bert):\r\n",
        "        super(BERT_Arch, self).__init__()\r\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\r\n",
        "        self.conv = nn.Conv2d(in_channels=13, out_channels=13, kernel_size=(3, 768), padding=True)\r\n",
        "        self.relu = nn.ReLU()\r\n",
        "        self.pool = nn.MaxPool2d(kernel_size=3, stride=1)\r\n",
        "        self.dropout = nn.Dropout(0.1)\r\n",
        "        self.fc = nn.Linear(442, 3) # before : 442 with max_length 36 # 806 with max_length 64\r\n",
        "        self.flat = nn.Flatten()\r\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\r\n",
        "\r\n",
        "    def forward(self, sent_id, mask):\r\n",
        "        _, _, all_layers = self.bert(sent_id, attention_mask=mask, output_hidden_states=True)\r\n",
        "        # all_layers  = [13, 32, 64, 768]\r\n",
        "        x = torch.transpose(torch.cat(tuple([t.unsqueeze(0) for t in all_layers]), 0), 0, 1)\r\n",
        "        del all_layers\r\n",
        "        gc.collect()\r\n",
        "        torch.cuda.empty_cache()\r\n",
        "        x = self.pool(self.dropout(self.relu(self.conv(self.dropout(x)))))\r\n",
        "        x = self.fc(self.dropout(self.flat(self.dropout(x))))\r\n",
        "        return self.softmax(x)\r\n",
        "\r\n",
        "\r\n",
        "def read_dataset():\r\n",
        "    data = pd.read_csv(\"/content/drive/MyDrive/labeled_data.csv\")\r\n",
        "    data = data.drop(['count', 'hate_speech', 'offensive_language', 'neither'], axis=1)\r\n",
        "    #data = data.loc[0:9599,:]\r\n",
        "    print(len(data))\r\n",
        "    return data['tweet'].tolist(), data['class']\r\n",
        "\r\n",
        "\r\n",
        "def pre_process_dataset(values):\r\n",
        "    new_values = list()\r\n",
        "    # Emoticons\r\n",
        "    emoticons = [':-)', ':)', '(:', '(-:', ':))', '((:', ':-D', ':D', 'X-D', 'XD', 'xD', 'xD', '<3', '</3', ':\\*',\r\n",
        "                 ';-)',\r\n",
        "                 ';)', ';-D', ';D', '(;', '(-;', ':-(', ':(', '(:', '(-:', ':,(', ':\\'(', ':\"(', ':((', ':D', '=D',\r\n",
        "                 '=)',\r\n",
        "                 '(=', '=(', ')=', '=-O', 'O-=', ':o', 'o:', 'O:', 'O:', ':-o', 'o-:', ':P', ':p', ':S', ':s', ':@',\r\n",
        "                 ':>',\r\n",
        "                 ':<', '^_^', '^.^', '>.>', 'T_T', 'T-T', '-.-', '*.*', '~.~', ':*', ':-*', 'xP', 'XP', 'XP', 'Xp',\r\n",
        "                 ':-|',\r\n",
        "                 ':->', ':-<', '$_$', '8-)', ':-P', ':-p', '=P', '=p', ':*)', '*-*', 'B-)', 'O.o', 'X-(', ')-X']\r\n",
        "\r\n",
        "    for value in values:\r\n",
        "        # Remove dots\r\n",
        "        text = value.replace(\".\", \"\").lower()\r\n",
        "        text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\r\n",
        "        users = re.findall(\"[@]\\w+\", text)\r\n",
        "        for user in users:\r\n",
        "            text = text.replace(user, \"<user>\")\r\n",
        "        urls = re.findall(r'(https?://[^\\s]+)', text)\r\n",
        "        if len(urls) != 0:\r\n",
        "            for url in urls:\r\n",
        "                text = text.replace(url, \"<url >\")\r\n",
        "        for emo in text:\r\n",
        "            if emo in emoji.UNICODE_EMOJI:\r\n",
        "                text = text.replace(emo, \"<emoticon >\")\r\n",
        "        for emo in emoticons:\r\n",
        "            text = text.replace(emo, \"<emoticon >\")\r\n",
        "        numbers = re.findall('[0-9]+', text)\r\n",
        "        for number in numbers:\r\n",
        "            text = text.replace(number, \"<number >\")\r\n",
        "        text = text.replace('#', \"<hashtag >\")\r\n",
        "        text = re.sub(r\"([?.!,¿])\", r\" \", text)\r\n",
        "        text = \"\".join(l for l in text if l not in string.punctuation)\r\n",
        "        text = re.sub(r'[\" \"]+', \" \", text)\r\n",
        "        new_values.append(text)\r\n",
        "    return new_values\r\n",
        "\r\n",
        "\r\n",
        "def data_process(data, labels):\r\n",
        "    input_ids = []\r\n",
        "    attention_masks = []\r\n",
        "    bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\r\n",
        "    for sentence in data:\r\n",
        "        bert_inp = bert_tokenizer.__call__(sentence, max_length=36,\r\n",
        "                                           padding='max_length', pad_to_max_length=True,\r\n",
        "                                           truncation=True, return_token_type_ids=False)\r\n",
        "\r\n",
        "        input_ids.append(bert_inp['input_ids'])\r\n",
        "        attention_masks.append(bert_inp['attention_mask'])\r\n",
        "    #del bert_tokenizer\r\n",
        "    #gc.collect()\r\n",
        "    #torch.cuda.empty_cache()\r\n",
        "    input_ids = np.asarray(input_ids)\r\n",
        "    attention_masks = np.array(attention_masks)\r\n",
        "    labels = np.array(labels)\r\n",
        "    return input_ids, attention_masks, labels\r\n",
        "\r\n",
        "\r\n",
        "def load_and_process():\r\n",
        "    data, labels = read_dataset()\r\n",
        "    num_of_labels = len(labels.unique())\r\n",
        "    input_ids, attention_masks, labels = data_process(pre_process_dataset(data), labels)\r\n",
        "\r\n",
        "    return input_ids, attention_masks, labels\r\n",
        "\r\n",
        "\r\n",
        "# function to train the model\r\n",
        "def train():\r\n",
        "    model.train()\r\n",
        "\r\n",
        "    total_loss, total_accuracy = 0, 0\r\n",
        "\r\n",
        "    # empty list to save model predictions\r\n",
        "    total_preds = []\r\n",
        "\r\n",
        "    # iterate over batches\r\n",
        "    total = len(train_dataloader)\r\n",
        "    for i, batch in enumerate(train_dataloader):\r\n",
        "\r\n",
        "        step = i+1\r\n",
        "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\r\n",
        "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\r\n",
        "        filledLength = int(100 * step // total)\r\n",
        "        bar = '█' * filledLength + '>'  *(filledLength < 100) + '.' * (99 - filledLength)\r\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\r\n",
        "\r\n",
        "        # push the batch to gpu\r\n",
        "        batch = [r.to(device) for r in batch]\r\n",
        "        sent_id, mask, labels = batch\r\n",
        "        del batch\r\n",
        "        gc.collect()\r\n",
        "        torch.cuda.empty_cache()\r\n",
        "        # clear previously calculated gradients\r\n",
        "        model.zero_grad()\r\n",
        "\r\n",
        "        # get model predictions for the current batch\r\n",
        "        #sent_id = torch.tensor(sent_id).to(device).long()\r\n",
        "        preds = model(sent_id, mask)\r\n",
        "\r\n",
        "        # compute the loss between actual and predicted values\r\n",
        "        loss = cross_entropy(preds, labels)\r\n",
        "\r\n",
        "        # add on to the total loss\r\n",
        "        total_loss += float(loss.item())\r\n",
        "\r\n",
        "        # backward pass to calculate the gradients\r\n",
        "        loss.backward()\r\n",
        "\r\n",
        "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\r\n",
        "\r\n",
        "        # update parameters\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        # model predictions are stored on GPU. So, push it to CPU\r\n",
        "        #preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "        # append the model predictions\r\n",
        "        #total_preds.append(preds)\r\n",
        "        total_preds.append(preds.detach().cpu().numpy())\r\n",
        "\r\n",
        "    gc.collect()\r\n",
        "    torch.cuda.empty_cache()\r\n",
        "\r\n",
        "    # compute the training loss of the epoch\r\n",
        "    avg_loss = total_loss / (len(train_dataloader)*batch_size)\r\n",
        "\r\n",
        "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\r\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "    # returns the loss and predictions\r\n",
        "    return avg_loss, total_preds\r\n",
        "\r\n",
        "\r\n",
        "# function for evaluating the model\r\n",
        "def evaluate():\r\n",
        "    print(\"\\n\\nEvaluating...\")\r\n",
        "\r\n",
        "    # deactivate dropout layers\r\n",
        "    model.eval()\r\n",
        "\r\n",
        "    total_loss, total_accuracy = 0, 0\r\n",
        "\r\n",
        "    # empty list to save the model predictions\r\n",
        "    total_preds = []\r\n",
        "\r\n",
        "    # iterate over batches\r\n",
        "    total = len(val_dataloader)\r\n",
        "    for i, batch in enumerate(val_dataloader):\r\n",
        "        \r\n",
        "        step = i+1\r\n",
        "        percent = \"{0:.2f}\".format(100 * (step / float(total)))\r\n",
        "        lossp = \"{0:.2f}\".format(total_loss/(total*batch_size))\r\n",
        "        filledLength = int(100 * step // total)\r\n",
        "        bar = '█' * filledLength + '>' * (filledLength < 100) + '.' * (99 - filledLength)\r\n",
        "        print(f'\\rBatch {step}/{total} |{bar}| {percent}% complete, loss={lossp}, accuracy={total_accuracy}', end='')\r\n",
        "\r\n",
        "        # push the batch to gpu\r\n",
        "        batch = [t.to(device) for t in batch]\r\n",
        "\r\n",
        "        sent_id, mask, labels = batch\r\n",
        "        del batch\r\n",
        "        gc.collect()\r\n",
        "        torch.cuda.empty_cache()\r\n",
        "        # deactivate autograd\r\n",
        "        with torch.no_grad():\r\n",
        "\r\n",
        "            # model predictions\r\n",
        "            preds = model(sent_id, mask)\r\n",
        "\r\n",
        "            # compute the validation loss between actual and predicted values\r\n",
        "            loss = cross_entropy(preds, labels)\r\n",
        "\r\n",
        "            total_loss += float(loss.item())\r\n",
        "            #preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "            #total_preds.append(preds)\r\n",
        "            total_preds.append(preds.detach().cpu().numpy())\r\n",
        "\r\n",
        "    gc.collect()\r\n",
        "    torch.cuda.empty_cache()\r\n",
        "\r\n",
        "    # compute the validation loss of the epoch\r\n",
        "    avg_loss = total_loss / (len(val_dataloader)*batch_size)\r\n",
        "\r\n",
        "    # reshape the predictions in form of (number of samples, no. of classes)\r\n",
        "    total_preds = np.concatenate(total_preds, axis=0)\r\n",
        "\r\n",
        "    return avg_loss, total_preds\r\n",
        "\r\n",
        "# Specify the GPU\r\n",
        "# Setting up the device for GPU usage\r\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
        "print(device)\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Load Data-set ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "input_ids, attention_masks, labels = load_and_process()\r\n",
        "df = pd.DataFrame(list(zip(input_ids, attention_masks)), columns=['input_ids', 'attention_masks'])\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ class distribution ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "\r\n",
        "# class = class label for majority of CF users. 0 - hate speech 1 - offensive language 2 - neither\r\n",
        "# ~~~~~~~~~~ Split train data-set into train, validation and test sets  ~~~~~~~~~~#\r\n",
        "train_text, temp_text, train_labels, temp_labels = train_test_split(df, labels,\r\n",
        "                             random_state=2018, test_size=0.2, stratify=labels)\r\n",
        "\r\n",
        "val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,\r\n",
        "                         random_state=2018, test_size=0.5, stratify=temp_labels)\r\n",
        "\r\n",
        "del temp_text\r\n",
        "gc.collect()\r\n",
        "torch.cuda.empty_cache()\r\n",
        "\r\n",
        "train_count = len(train_labels)\r\n",
        "test_count = len(test_labels)\r\n",
        "val_count = len(val_labels)\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~ Import BERT Model and BERT Tokenizer ~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "# import BERT-base pretrained model\r\n",
        "bert = AutoModel.from_pretrained('bert-base-uncased')\r\n",
        "# bert = AutoModel.from_pretrained('bert-base-uncased')\r\n",
        "# Load the BERT tokenizer\r\n",
        "#tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Tokenization ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "# for train set\r\n",
        "train_seq = torch.tensor(train_text['input_ids'].tolist())\r\n",
        "train_mask = torch.tensor(train_text['attention_masks'].tolist())\r\n",
        "train_y = torch.tensor(train_labels.tolist())\r\n",
        "\r\n",
        "# for validation set\r\n",
        "val_seq = torch.tensor(val_text['input_ids'].tolist())\r\n",
        "val_mask = torch.tensor(val_text['attention_masks'].tolist())\r\n",
        "val_y = torch.tensor(val_labels.tolist())\r\n",
        "\r\n",
        "# for test set\r\n",
        "test_seq = torch.tensor(test_text['input_ids'].tolist())\r\n",
        "test_mask = torch.tensor(test_text['attention_masks'].tolist())\r\n",
        "test_y = torch.tensor(test_labels.tolist())\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Create DataLoaders ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\r\n",
        "\r\n",
        "# define a batch size\r\n",
        "batch_size = 32\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "train_data = TensorDataset(train_seq, train_mask, train_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "train_sampler = RandomSampler(train_data)\r\n",
        "\r\n",
        "# dataLoader for train set\r\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\r\n",
        "\r\n",
        "# wrap tensors\r\n",
        "val_data = TensorDataset(val_seq, val_mask, val_y)\r\n",
        "\r\n",
        "# sampler for sampling the data during training\r\n",
        "val_sampler = SequentialSampler(val_data)\r\n",
        "\r\n",
        "# dataLoader for validation set\r\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Freeze BERT Parameters ~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "# freeze all the parameters\r\n",
        "for param in bert.parameters():\r\n",
        "    param.requires_grad = False\r\n",
        "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~#\r\n",
        "\r\n",
        "# pass the pre-trained BERT to our define architecture\r\n",
        "model = BERT_Arch(bert)\r\n",
        "# push the model to GPU\r\n",
        "model = model.to(device)\r\n",
        "\r\n",
        "# optimizer from hugging face transformers\r\n",
        "from transformers import AdamW\r\n",
        "\r\n",
        "# define the optimizer\r\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\r\n",
        "\r\n",
        "#from sklearn.utils.class_weight import compute_class_weight\r\n",
        "\r\n",
        "# compute the class weights\r\n",
        "#class_wts = compute_class_weight('balanced', np.unique(train_labels), train_labels)\r\n",
        "\r\n",
        "#print(class_wts)\r\n",
        "\r\n",
        "# convert class weights to tensor\r\n",
        "#weights = torch.tensor(class_wts, dtype=torch.float)\r\n",
        "#weights = weights.to(device)\r\n",
        "\r\n",
        "# loss function\r\n",
        "#cross_entropy = nn.NLLLoss(weight=weights)\r\n",
        "cross_entropy = nn.NLLLoss()\r\n",
        "\r\n",
        "# set initial loss to infinite\r\n",
        "best_valid_loss = float('inf')\r\n",
        "\r\n",
        "# empty lists to store training and validation loss of each epoch\r\n",
        "#train_losses = []\r\n",
        "#valid_losses = []\r\n",
        "\r\n",
        "#if os.path.isfile(\"/content/drive/MyDrive/saved_weights.pth\") == False:\r\n",
        "#if os.path.isfile(\"saved_weights.pth\") == False:\r\n",
        "    # number of training epochs\r\n",
        "epochs = 3\r\n",
        "current = 1\r\n",
        "# for each epoch\r\n",
        "while current <= epochs:\r\n",
        "\r\n",
        "    print(f'\\nEpoch {current} / {epochs}:')\r\n",
        "\r\n",
        "    # train model\r\n",
        "    train_loss, _ = train()\r\n",
        "\r\n",
        "    # evaluate model\r\n",
        "    valid_loss, _ = evaluate()\r\n",
        "\r\n",
        "    # save the best model\r\n",
        "    if valid_loss < best_valid_loss:\r\n",
        "        best_valid_loss = valid_loss\r\n",
        "        #torch.save(model.state_dict(), 'saved_weights.pth')\r\n",
        "\r\n",
        "    # append training and validation loss\r\n",
        "    #train_losses.append(train_loss)\r\n",
        "    #valid_losses.append(valid_loss)\r\n",
        "\r\n",
        "    print(f'\\n\\nTraining Loss: {train_loss:.3f}')\r\n",
        "    print(f'Validation Loss: {valid_loss:.3f}')\r\n",
        "\r\n",
        "    current = current + 1\r\n",
        "#else:\r\n",
        "    #print(\"Got weights!\")\r\n",
        "    # load weights of best model\r\n",
        "    #model.load_state_dict(torch.load(\"saved_weights.pth\"))\r\n",
        "    #model.load_state_dict(torch.load(\"/content/drive/MyDrive/saved_weights.pth\"), strict=False)\r\n",
        "\r\n",
        "# get predictions for test data\r\n",
        "gc.collect()\r\n",
        "torch.cuda.empty_cache()\r\n",
        "\r\n",
        "with torch.no_grad():\r\n",
        "    preds = model(test_seq.to(device), test_mask.to(device))\r\n",
        "    #preds = model(test_seq, test_mask)\r\n",
        "    preds = preds.detach().cpu().numpy()\r\n",
        "\r\n",
        "\r\n",
        "print(\"Performance:\")\r\n",
        "# model's performance\r\n",
        "preds = np.argmax(preds, axis=1)\r\n",
        "print('Classification Report')\r\n",
        "print(classification_report(test_y, preds))\r\n",
        "\r\n",
        "print(\"Accuracy: \" + str(accuracy_score(test_y, preds)))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.6/dist-packages (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (20.8)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (3.0.12)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.1.94)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (1.19.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.0.0) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.0.0) (1.0.0)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.6/dist-packages (0.6.0)\n",
            "cuda\n",
            "24783\n",
            "\n",
            "Epoch 1 / 3:\n",
            "Batch 620/620 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n",
            "\n",
            "Evaluating...\n",
            "Batch 78/78 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n",
            "\n",
            "Training Loss: 0.010\n",
            "Validation Loss: 0.007\n",
            "\n",
            "Epoch 2 / 3:\n",
            "Batch 620/620 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n",
            "\n",
            "Evaluating...\n",
            "Batch 78/78 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n",
            "\n",
            "Training Loss: 0.007\n",
            "Validation Loss: 0.008\n",
            "\n",
            "Epoch 3 / 3:\n",
            "Batch 620/620 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n",
            "\n",
            "Evaluating...\n",
            "Batch 78/78 |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00% complete, loss=0.01, accuracy=0\n",
            "\n",
            "Training Loss: 0.005\n",
            "Validation Loss: 0.009\n",
            "Performance:\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.49      0.45      0.47       143\n",
            "           1       0.95      0.96      0.95      1919\n",
            "           2       0.90      0.88      0.89       417\n",
            "\n",
            "    accuracy                           0.91      2479\n",
            "   macro avg       0.78      0.77      0.77      2479\n",
            "weighted avg       0.91      0.91      0.91      2479\n",
            "\n",
            "Accuracy: 0.9148850342880194\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}